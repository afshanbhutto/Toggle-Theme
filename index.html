<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="ie=edge">
    <link href="https://fonts.googleapis.com/css2?family=Dosis:wght@523&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.13.0/css/all.css" integrity="sha384-Bfad6CLCknfcloXFOyFnlgtENryhrpZCe29RTifKEixXQZ38WheV+i/6YWSzkz3V" crossorigin="anonymous">
    <title>Toggle Theme</title>
    <link rel="stylesheet" href="style.css">
</head>
<body>
    <div class="container">
    <div>
        <input type="checkbox" class="checkbox" id="checkbox">
        <label for="checkbox" class="label">
            <i class="fas fa-moon"></i>
            <i class="fas fa-sun"></i>
            <div class="circle"></div>
        </label>
    </div>
   
    <div class="text">
            
        <p>
            Closures are confusing because they are an “invisible” concept.
        </p>

        <p>
            When you use an object, a variable, or a function, you do this intentionally. You think: “I’m gonna need a variable here,” and add it to your code.
        </p>

        <p>
            Closures are different. By the time most people approach closures, they have already used them unknowingly many times — and it is likely that this is true for yourself, too. So learning closures is less about understanding a new concept and more about recognizing something you have already been doing for a while.

        </p>

        <h1 class="heading">GPT-3</h1>

        <p class="blue">
            GPT-3 is a neural-network-powered language model. A language model is a model that predicts the likelihood of a sentence existing in the world.
        </p>

        <p>
            The GPT-3 model architecture itself is a transformer-based neural network. This architecture became popular around 2–3 years ago, and is the basis for the popular NLP model BERT and GPT-3’s predecessor, GPT-2. From an architecture perspective, GPT-3 is not actually very novel!
        </p>

        <p class="blue">
            IT’S REALLY BIG. I mean really big. With 175 billion parameters, it’s the largest language model ever created
        </p>
    

        <p>
            You can ask GPT-3 to be a translator, a programmer, a poet, or a famous author, and it can do it with its user (you) providing fewer than 10 training examples. Damn.
        </p>
    

        <p class="blue">
            Today, GPT-3 is in private beta, but boy can I not wait to get my hands on it.
        </p>
    </div>

</div>

    <script src="script.js"></script>
</body>
</html>